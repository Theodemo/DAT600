# 1. Introduction aux Algorithmes

## 1.1 D√©finition et Importance des Algorithmes

Un **algorithme** est une suite finie d'instructions bien d√©finies permettant de r√©soudre un probl√®me ou d'accomplir une t√¢che. Il peut √™tre exprim√© sous diff√©rentes formes : langage naturel, pseudo-code, diagramme de flux ou langage de programmation.

### **Importance des Algorithmes**
- **Efficacit√©** : Permet de r√©soudre des probl√®mes complexes en un temps raisonnable.
- **Optimisation** : R√©duction du temps d'ex√©cution et de la consommation des ressources.
- **Automatisation** : Ex√©cution r√©p√©t√©e et fiable sans intervention humaine.
- **Universalit√©** : Application dans divers domaines (informatique, math√©matiques, physique, etc.).

## 1.2 Historique et √âvolution

### **Les Premiers Algorithmes**
- **Antiquit√©** : Algorithme d'Euclide (~300 av. J.-C.) pour le calcul du PGCD.
- **Moyen √Çge** : Al-Khw√¢rizm√Æ (~IXe si√®cle), dont le nom a donn√© naissance au terme "algorithme".
- **XVIIe - XIXe si√®cle** : D√©veloppement des math√©matiques algorithmiques (Newton, Gauss, etc.).

### **√àre Moderne**
- **1936** : Alan Turing propose la machine de Turing, mod√®le abstrait du calcul.
- **1940-1950** : D√©veloppement des premiers ordinateurs et langages de programmation.
- **1970-1980** : Apparition des concepts de complexit√© algorithmique et classification des probl√®mes.
- **1990 - Aujourd'hui** : Avanc√©es en intelligence artificielle, algorithmes distribu√©s et quantiques.

## 1.3 Notion d‚ÄôEfficacit√© et d‚ÄôOptimalit√©

### **Efficacit√© d'un Algorithme**
L'efficacit√© d'un algorithme est mesur√©e en fonction de :
- **Complexit√© en temps** : Nombre d'op√©rations n√©cessaires pour ex√©cuter l'algorithme.
- **Complexit√© en espace** : Quantit√© de m√©moire utilis√©e.

### **Optimalit√©**
Un algorithme est **optimal** s'il r√©sout un probl√®me en un minimum de ressources possibles (temps et espace). On √©value l'optimalit√© gr√¢ce aux bornes inf√©rieures connues du probl√®me.

### **Exemple : Tri de Tableaux**
- **Tri par s√©lection** : O(n¬≤) en temps, peu optimal pour les grandes donn√©es.
- **Tri fusion** : O(n log n), plus efficace pour de grandes entr√©es.

L‚Äô√©tude de ces notions est essentielle pour concevoir des solutions informatiques performantes et adapt√©es aux besoins r√©els.

# 2. Mod√®les de Calcul

Les mod√®les de calcul sont des abstractions math√©matiques permettant de formaliser la notion de calcul et de comprendre les limites de ce qui peut √™tre calcul√© par une machine. Parmi ces mod√®les, la **Machine de Turing** occupe une place centrale.

## 2.1. Machine de Turing

La **Machine de Turing** est un mod√®le de calcul th√©orique propos√© par **Alan Turing** en 1936. Elle joue un r√¥le fondamental dans la th√©orie de la calculabilit√© et la complexit√© des algorithmes.

### 2.1.1. D√©finition

Une machine de Turing est d√©finie comme une s√©quence de composants suivants :

- **Un ruban infini** divis√© en cases contenant des symboles.
- **Une t√™te de lecture/√©criture**, capable de lire et d'√©crire des symboles sur le ruban.
- **Un ensemble d'√©tats internes** parmi lesquels un √©tat initial et des √©tats d'acceptation ou de rejet.
- **Une table de transition**, dictant les actions √† entreprendre en fonction de l'√©tat courant et du symbole lu.

### 2.1.2. Fonctionnement

Le fonctionnement de la machine de Turing repose sur les √©tapes suivantes :

1. La t√™te lit un symbole sur le ruban.
2. En fonction de l'√©tat courant et du symbole lu, la machine :
   - Modifie le symbole courant (ou le laisse inchang√©).
   - D√©place la t√™te vers la gauche ou la droite.
   - Change d'√©tat interne.
3. Le processus se poursuit jusqu'√† atteindre un √©tat d'acceptation ou de rejet.

### 2.1.3. Importance et Applications

- **Th√©orie de la calculabilit√©** : La machine de Turing permet de d√©finir ce qui est **calculable** et ce qui ne l'est pas.
- **Complexit√© algorithmique** : Elle sert de base √† l'√©tude des classes de complexit√© (P, NP, etc.).
- **Langages formels** : Elle est utilis√©e pour mod√©liser les langages r√©cursivement √©num√©rables.
- **Informatique th√©orique** : De nombreux mod√®les modernes de calcul d√©coulent des concepts de la machine de Turing.

### 2.1.4. Variantes de la Machine de Turing

- **Machine de Turing multi-rubans** : Utilise plusieurs rubans pour augmenter l'efficacit√©.
- **Machine de Turing non d√©terministe** : Peut effectuer plusieurs transitions simultan√©ment.
- **Machine de Turing universelle** : Capable de simuler toute autre machine de Turing, fondement des ordinateurs modernes.

---

La machine de Turing constitue un outil essentiel pour comprendre les fondements de l'informatique et les limites du calcul. Elle reste une r√©f√©rence incontournable en th√©orie des algorithmes.
## 2.2. Automates et langages formels

### 2.2.1. Introduction aux langages formels
Un **langage formel** est un ensemble de cha√Ænes de caract√®res construites √† partir d'un alphabet donn√©. L'√©tude des langages formels est fondamentale en informatique th√©orique, notamment pour la compilation et la reconnaissance de motifs.

### 2.2.2. Les Alphabets, Mots et Langages
- **Alphabet (\(\Sigma\))** : Un ensemble fini de symboles.
- **Mot** : Une suite finie de symboles appartenant √† un alphabet.
- **Langage** : Un sous-ensemble de l‚Äôensemble de tous les mots possibles sur un alphabet donn√© (\(\Sigma^*\)).

Exemple :
Si \(\Sigma = \{a, b\}\), alors quelques mots possibles sont \("a"\), \("b"\), \("ab"\), \("ba"\), etc.

### 2.2.3. Classes de Langages
Les langages sont class√©s selon la **hi√©rarchie de Chomsky** :
1. **Langages r√©guliers** (Reconnaissables par des automates finis)
2. **Langages contextuels** (Reconnaissables par des automates √† pile)
3. **Langages sensibles au contexte**
4. **Langages r√©cursivement √©num√©rables** (Reconnaissables par une machine de Turing)

### 2.2.4. Automates Finis
Un **automate fini** est un mod√®le math√©matique permettant de repr√©senter des syst√®mes √† √©tats finis. Il est d√©fini par un quintuplet \( (Q, \Sigma, \delta, q_0, F) \) o√π :
- \(Q\) : Ensemble fini d‚Äô√©tats
- \(\Sigma\) : Alphabet d‚Äôentr√©e
- \(\delta\) : Fonction de transition \( Q \times \Sigma \to Q \)
- \(q_0\) : √âtat initial
- \(F\) : Ensemble d‚Äô√©tats acceptants

Il existe deux types d‚Äôautomates :
- **Automates finis d√©terministes (DFA)** : Une seule transition possible par symbole.
- **Automates finis non d√©terministes (NFA)** : Plusieurs transitions possibles.

Exemple de DFA :
```
      a       b
  --> (q0) --- a ---> (q1)
```

### 2.2.5. Expressions R√©guli√®res et Automates Finis
Les **expressions r√©guli√®res** sont utilis√©es pour d√©crire les langages r√©guliers. Par exemple :
- \(a^*b\) repr√©sente des mots contenant z√©ro ou plusieurs \("a"\) suivis d‚Äôun \("b"\).

### 2.2.6. Automates √† Pile et Langages Contextuels
Un **automate √† pile** est une extension d‚Äôun automate fini qui poss√®de une m√©moire sous forme de **pile**. Il permet de reconna√Ætre les langages contextuels d√©finis par une **grammaire hors-contexte**.

Exemple de langage hors-contexte :
- \(L = \{a^n b^n | n \geq 0\}\), qui ne peut pas √™tre reconnu par un automate fini, mais l‚Äôest par un automate √† pile.

### 2.2.7. Machines de Turing et Langages R√©cursivement √ânum√©rables
Une **machine de Turing** est un mod√®le plus puissant capable de reconna√Ætre des langages plus complexes. Elle est d√©finie par :
- Une bande infinie pour la m√©moire
- Une t√™te de lecture/√©criture
- Un ensemble fini d‚Äô√©tats et une fonction de transition

Les machines de Turing sont essentielles pour d√©finir la **d√©cidabilit√©** et la **calculabilit√©** en informatique.

### 2.2.8. Applications des Automates et Langages Formels
- **Compilateurs** : Analyse lexicale et syntaxique
- **Moteurs de recherche** : Traitement des requ√™tes
- **S√©curit√© informatique** : D√©tection de patterns malveillants
- **Traitement du langage naturel** : Reconnaissance de la syntaxe


## 2.3. Mod√®le RAM (Random Access Machine)

Le **mod√®le RAM (Random Access Machine)** est un mod√®le abstrait de calcul utilis√© pour analyser la complexit√© des algorithmes en consid√©rant un ordinateur id√©al disposant d'une m√©moire infinie et d'un processeur ex√©cutant des instructions √©l√©mentaires en temps constant.

### 2.3.1. D√©finition du Mod√®le RAM
Le mod√®le RAM repose sur plusieurs hypoth√®ses simplificatrices :
- La machine dispose d‚Äôune **m√©moire infinie** divis√©e en cellules accessibles par adresse.
- Chaque cellule peut contenir un entier ou une valeur de taille fixe.
- Les **op√©rations √©l√©mentaires** (addition, soustraction, multiplication, acc√®s m√©moire, etc.) s‚Äôex√©cutent en **temps constant**.
- Il n'y a pas de **gestion de cache**, de **hi√©rarchie m√©moire** ou de **parall√©lisme**.

### 2.3.2. Structure d‚Äôun Programme RAM
Un programme dans le mod√®le RAM est constitu√© d'une **suite d‚Äôinstructions**, similaires aux instructions d‚Äôun processeur classique :
- **Op√©rations arithm√©tiques** : addition, soustraction, multiplication, division.
- **Acc√®s m√©moire** : lecture et √©criture en m√©moire par adresse.
- **Instructions de contr√¥le** : conditions, boucles, branchements.
- **Op√©rations logiques** : ET, OU, NON.

### 2.3.3. Comparaison avec d‚ÄôAutres Mod√®les
| Mod√®le | Avantages | Inconv√©nients |
|--------|----------|--------------|
| **RAM** | Simple et intuitif pour l‚Äôanalyse de complexit√© | Ne prend pas en compte les contraintes mat√©rielles r√©elles |
| **Machine de Turing** | Th√©orique et universellement applicable | Peu pratique pour l‚Äôanalyse d‚Äôalgorithmes r√©els |
| **Mod√®le de calcul parall√®le** | Permet d‚Äô√©tudier les architectures modernes | Plus complexe que le mod√®le RAM |

### 2.3.4. Limites et Critiques
Bien que largement utilis√© en analyse algorithmique, le mod√®le RAM pr√©sente plusieurs **limitations** :
- **Supposition irr√©aliste** d‚Äôun temps d‚Äôex√©cution constant pour toutes les op√©rations.
- **Ignorance des contraintes mat√©rielles**, comme la hi√©rarchie de m√©moire (cache, RAM, disque).
- **Absence de parall√©lisme**, alors que la majorit√© des processeurs modernes sont multic≈ìurs.

### 2.3.5. Utilisation en Analyse de Complexit√©
Malgr√© ses limitations, le mod√®le RAM reste un outil fondamental pour :
- **Analyser la complexit√© des algorithmes** ind√©pendamment du mat√©riel.
- **√âtablir des bornes de performance** en utilisant les notations asymptotiques (O, Œ©, Œò).
- **Comparer diff√©rents algorithmes** dans un cadre standardis√©.

### 2.3.6. Conclusion
Le **mod√®le RAM** est un cadre simplifi√© mais puissant pour l‚Äôanalyse algorithmique. Il permet de comprendre et comparer les performances des algorithmes sans se soucier des d√©tails mat√©riels. Toutefois, dans des contextes pratiques, des mod√®les plus avanc√©s peuvent √™tre n√©cessaires pour une analyse plus fine.

# 3. Analyse de la Complexit√© des Algorithmes

L'analyse de la complexit√© des algorithmes permet d'√©valuer leurs performances en fonction de la taille de l'entr√©e. Elle est essentielle pour comparer diff√©rentes solutions √† un m√™me probl√®me et choisir l'algorithme le plus efficace. 

## 3.1. Notations asymptotiques (O, Œ©, Œò)

Les notations asymptotiques permettent de caract√©riser le comportement d'un algorithme lorsqu'il est appliqu√© √† des entr√©es de grande taille. Elles d√©finissent des bornes sur le temps d'ex√©cution ou l'utilisation de la m√©moire.

### **3.1.1. Notation O (O grand) : borne sup√©rieure**
La notation **O (big-O)** donne une borne sup√©rieure sur la croissance d'une fonction. Elle repr√©sente le pire cas d'ex√©cution d'un algorithme.

- Forme math√©matique :
  $$ f(n) = O(g(n)) \text{ si et seulement si } \exists c > 0, \exists n_0 > 0, \forall n \geq n_0, \quad f(n) \leq c \cdot g(n) $$
- Exemple :
  - Si un algorithme s'ex√©cute en **f(n) = 3n¬≤ + 2n + 5**, alors **f(n) = O(n¬≤)** car il existe une constante c telle que **f(n) ‚â§ c ‚ãÖ n¬≤** pour des valeurs suffisantes de n.

### **3.1.2. Notation Œ© (Om√©ga) : borne inf√©rieure**
La notation **Œ© (Omega)** donne une borne inf√©rieure sur la croissance d'une fonction. Elle repr√©sente le meilleur cas d'ex√©cution d'un algorithme.

- Forme math√©matique :
  $$ f(n) = Œ©(g(n)) \text{ si et seulement si } \exists c > 0, \exists n_0 > 0, \forall n \geq n_0, \quad f(n) \geq c \cdot g(n) $$
- Exemple :
  - Pour **f(n) = 5n + 10**, on peut dire que **f(n) = Œ©(n)** car il existe une constante c telle que **f(n) ‚â• c ‚ãÖ n**.

### **3.1.3. Notation Œò (Th√™ta) : borne serr√©e**
La notation **Œò (Theta)** donne une borne asymptotique **exacte** sur la croissance d'une fonction. Un algorithme est **Œò(g(n))** si et seulement si il est √† la fois **O(g(n))** et **Œ©(g(n))**.

- Forme math√©matique :
  $$ f(n) = Œò(g(n)) \text{ si et seulement si } \exists c_1, c_2 > 0, \exists n_0 > 0, \forall n \geq n_0, \quad c_1 \cdot g(n) \leq f(n) \leq c_2 \cdot g(n) $$
- Exemple :
  - Pour **f(n) = 4n¬≤ + 3n + 7**, on peut dire que **f(n) = Œò(n¬≤)** car il existe des constantes c1 et c2 telles que **c1 ‚ãÖ n¬≤ ‚â§ f(n) ‚â§ c2 ‚ãÖ n¬≤**.

### **3.1.4. Relation entre O, Œ© et Œò**

- **O(g(n))** : Donne une borne **sup√©rieure**
- **Œ©(g(n))** : Donne une borne **inf√©rieure**
- **Œò(g(n))** : Donne une borne **exacte** (si une fonction est √† la fois O et Œ© de la m√™me fonction g(n))

Exemple visuel de relation entre ces notations :

```
 O(n¬≤)      Œò(n¬≤)      Œ©(n¬≤)
  |---------|---------|
      Croissance de f(n)
```

En pratique, on utilise surtout **O(n)** pour exprimer la complexit√© d'un algorithme, car elle donne une estimation du pire cas.

## 3.2. Complexit√© en temps et en espace

### 3.2.1. D√©finition de la Complexit√©
La complexit√© d‚Äôun algorithme mesure les ressources qu‚Äôil consomme en fonction de la taille de l‚Äôentr√©e. Les deux principales mesures de complexit√© sont :

- **Complexit√© en temps** : mesure le nombre d‚Äôop√©rations √©l√©mentaires ex√©cut√©es.
- **Complexit√© en espace** : mesure la quantit√© de m√©moire utilis√©e.

### 3.2.2. Complexit√© en Temps

#### Notation asymptotique
Pour analyser la complexit√© en temps, on utilise des notations asymptotiques :

- **O(f(n))** : Bornes sup√©rieures (croissance maximale)
- **Œ©(f(n))** : Bornes inf√©rieures (croissance minimale)
- **Œò(f(n))** : Encadrement serr√© (croissance exacte)

#### Types de complexit√©s courantes
| Complexit√© | Notation | Exemple |
|------------|----------|----------------|
| Constante  | O(1)     | Acc√®s √† un √©l√©ment dans un tableau |
| Logarithmique | O(log n) | Recherche dichotomique |
| Lin√©aire   | O(n)     | Parcours d‚Äôun tableau |
| Quadratique | O(n¬≤)   | Algorithme de tri par insertion |
| Exponentielle | O(2‚Åø) | Algorithme de backtracking |

### 3.2.3. Complexit√© en Espace
La complexit√© en espace repr√©sente la quantit√© de m√©moire utilis√©e par un algorithme en fonction de la taille de l‚Äôentr√©e.

#### M√©moire utilis√©e
Elle se d√©compose en :
- **Espace fixe** : m√©moire utilis√©e ind√©pendamment de l‚Äôentr√©e (variables, constantes, code du programme).
- **Espace variable** : m√©moire d√©pendant de l‚Äôentr√©e (tableaux, structures de donn√©es dynamiques).

#### Exemples
| Algorithme | Complexit√© en espace |
|------------|--------------------|
| Recherche lin√©aire | O(1) |
| Tri fusion | O(n) |
| Algorithmes r√©cursifs | O(n) √† O(n¬≤) selon le stockage de l‚Äôappel r√©cursif |

### 3.2.4. Relation entre Complexit√© en Temps et en Espace
Il existe souvent un compromis entre l‚Äôespace et le temps. Par exemple, utiliser une table de hachage (O(1) en temps) peut n√©cessiter plus de m√©moire qu‚Äôune recherche lin√©aire (O(n) en temps mais O(1) en m√©moire).

### 3.2.5. Optimisation de la Complexit√©
- **Optimisation en temps** : Algorithmes efficaces (ex: tri fusion au lieu de tri bulle)
- **Optimisation en espace** : Structures de donn√©es adapt√©es (ex: utilisation d‚Äôarbres √©quilibr√©s au lieu de matrices pleines)

---

Cette section fournit un aper√ßu de la complexit√© en temps et en espace et souligne l‚Äôimportance d‚Äôune analyse approfondie pour choisir l‚Äôalgorithme le plus adapt√©.

## 3.3. Classes de complexit√© (P, NP, NP-complet, NP-dur)

### 3.3.1. Introduction aux Classes de Complexit√©
La th√©orie de la complexit√© vise √† classer les probl√®mes informatiques en fonction de la quantit√© de ressources (temps et espace) n√©cessaires pour les r√©soudre. Parmi ces classifications, les classes **P**, **NP**, **NP-complet** et **NP-dur** sont fondamentales en informatique th√©orique.

---

### 3.3.2. Classe P (Polynomial Time)
La classe **P** regroupe les probl√®mes qui peuvent √™tre r√©solus par un algorithme d√©terministe en **temps polynomial**.

- Un algorithme est dit **polynomial** s'il existe un entier **k** tel que le temps d'ex√©cution soit **O(n^k)**, o√π **n** est la taille de l'entr√©e.
- Ces probl√®mes sont consid√©r√©s comme **traitables efficacement**.

**Exemples de probl√®mes dans P :**
- Tri d'un tableau (**Tri Fusion, Tri Rapide**) ‚Üí **O(n log n)**
- Recherche du plus court chemin dans un graphe pond√©r√© (**Dijkstra**) ‚Üí **O(n^2)** ou **O(m + n log n)** avec tas de Fibonacci

---

### 3.3.3. Classe NP (Nondeterministic Polynomial Time)
La classe **NP** contient les probl√®mes pour lesquels une **solution peut √™tre v√©rifi√©e en temps polynomial** par un algorithme d√©terministe.

- Un probl√®me appartient √† **NP** si, √©tant donn√© une solution candidate, nous pouvons la v√©rifier en **O(n^k)**.
- Un algorithme non-d√©terministe pourrait r√©soudre ces probl√®mes en **temps polynomial** en utilisant la **devinette** d‚Äôune solution correcte.

**Exemples de probl√®mes dans NP :**
- **Probl√®me du Voyageur de Commerce (TSP)** : v√©rifier si un circuit d‚Äôun co√ªt donn√© existe prend un temps polynomial.
- **Probl√®me de la Satisfiabilit√© Bool√©enne (SAT)** : v√©rifier si une affectation satisfait une formule bool√©enne est faisable en temps polynomial.

---

### 3.3.4. Probl√®mes NP-complets (NP-C)
Un probl√®me est **NP-complet** s‚Äôil est **√† la fois dans NP et NP-dur**.

- Il s‚Äôagit des probl√®mes les plus difficiles de **NP**.
- Si un algorithme polynomial √©tait trouv√© pour un probl√®me NP-complet, alors **P = NP** (ce qui reste une question ouverte en informatique th√©orique).

**M√©thode pour prouver qu‚Äôun probl√®me est NP-complet :**
1. Montrer qu'il est dans **NP** (v√©rification en temps polynomial possible).
2. Faire une **r√©duction polynomiale** depuis un probl√®me NP-complet connu.

**Exemples de probl√®mes NP-complets :**
- **SAT (Satisfiabilit√© Bool√©enne)** (le premier probl√®me prouv√© NP-complet, th√©or√®me de Cook).
- **Probl√®me du Sac √† Dos** (optimisation combinatoire).
- **Probl√®me du Voyageur de Commerce (TSP, version d√©cisionnelle)**.

---

### 3.3.5. Probl√®mes NP-durs (NP-Hard)
Un probl√®me est **NP-dur** s‚Äôil est **au moins aussi difficile** que les probl√®mes de **NP**, mais il **n'a pas n√©cessairement besoin d'appartenir √† NP**.

- Ces probl√®mes peuvent √™tre **plus difficiles** que NP-complets.
- Ils peuvent ne pas avoir de solutions v√©rifiables en temps polynomial.
- Certains peuvent m√™me √™tre **ind√©cidables**.

**Exemples de probl√®mes NP-durs :**
- **Probl√®me du Voyageur de Commerce (TSP, version optimisation)**.
- **Probl√®me d‚Äôoptimisation du Sac √† Dos**.
- **R√©solution g√©n√©rale des jeux vid√©o (ex: √âchecs, Go, Sudoku avec une grille infinie)**.

---

### 3.3.6. Sch√©ma R√©sum√© des Relations entre P, NP, NP-complet et NP-dur
```
      P ‚äÜ NP
      NP-complet ‚äÜ NP
      NP-dur ‚äá NP-complet
```
- Tous les probl√®mes de **P** sont dans **NP**.
- Les **probl√®mes NP-complets** sont les plus difficiles de **NP**.
- Les **probl√®mes NP-durs** peuvent √™tre **hors de NP**, car ils ne n√©cessitent pas d‚Äôavoir une solution v√©rifiable en temps polynomial.

---

### 3.3.7. Conclusion
L‚Äô√©tude de ces classes de complexit√© est cruciale pour comprendre la difficult√© intrins√®que des probl√®mes et d√©terminer s‚Äôils sont solubles efficacement. La question **P = NP** reste l‚Äôun des plus grands myst√®res en math√©matiques et en informatique th√©orique.

## 3.4. R√©duction polynomiale et probl√®mes NP-complets

### 3.4.1. D√©finition de la r√©duction polynomiale
La **r√©duction polynomiale** est une technique utilis√©e en th√©orie de la complexit√© pour comparer la difficult√© des probl√®mes algorithmiques. Un probl√®me \( A \) est dit **r√©ductible polynomialement** √† un probl√®me \( B \) (not√© \( A \leq_P B \)) s'il existe une fonction \( f \) calculable en temps polynomial telle que :

\[ x \in A \iff f(x) \in B \]

Cela signifie que si nous savons r√©soudre \( B \) efficacement, alors nous pouvons √©galement r√©soudre \( A \) en transformant ses instances en instances de \( B \) via \( f \).

### 3.4.2. Importance de la r√©duction polynomiale
La r√©duction polynomiale joue un r√¥le crucial dans la classification des probl√®mes en **NP-complets**. Pour montrer qu'un probl√®me est NP-complet, il faut :

1. Montrer qu'il appartient √† la classe NP (on peut v√©rifier une solution en temps polynomial).
2. Montrer qu'un probl√®me d√©j√† connu comme NP-complet peut √™tre r√©duit polynomialement √† ce probl√®me.

Si ces deux conditions sont satisfaites, alors le probl√®me est aussi difficile que tous les autres probl√®mes NP-complets.

### 3.4.3. Exemples de probl√®mes NP-complets
Voici quelques probl√®mes classiques prouv√©s NP-complets via r√©duction polynomiale :

- **Probl√®me du voyageur de commerce (TSP) :** Donn√©e une liste de villes et les distances entre elles, trouver le plus court chemin qui visite chaque ville exactement une fois et revient au point de d√©part.
- **Probl√®me de la couverture de sommets :** Trouver un sous-ensemble minimal de sommets couvrant toutes les ar√™tes d'un graphe.
- **Probl√®me de la satisfaction bool√©enne (SAT) :** Trouver une affectation des variables qui satisfait une formule bool√©enne donn√©e.
- **Probl√®me du sac √† dos (Knapsack) :** S√©lectionner des objets avec une valeur et un poids, de sorte √† maximiser la valeur totale sans d√©passer une capacit√© donn√©e.

### 3.4.4. Cons√©quences et impact
La notion de r√©duction polynomiale est essentielle car elle permet d'identifier des probl√®mes difficiles en pratique. Si un probl√®me est NP-complet, alors sauf si **P = NP**, il n'existe pas d'algorithme en temps polynomial pour le r√©soudre dans le cas g√©n√©ral. 

Cela guide la recherche vers des approches alternatives comme :
- **Heuristiques** : Algorithmes qui donnent des solutions approximatives en un temps raisonnable.
- **Algorithmes d'approximation** : Garantissent une solution proche de l‚Äôoptimum avec une borne sur l‚Äôerreur.
- **M√©thodes exactes exponentielles** : Comme la programmation dynamique et le branch-and-bound.

### 3.4.5. Conclusion
La r√©duction polynomiale est un outil fondamental en informatique th√©orique. Elle permet d'identifier des probl√®mes difficiles et de mieux comprendre la structure de la classe NP. Si un jour un algorithme polynomial est d√©couvert pour un probl√®me NP-complet, alors **P = NP**, ce qui r√©volutionnerait l'informatique et la cryptographie !

# 4. Paradigmes Algorithmiques

Les **paradigmes algorithmiques** sont des strat√©gies g√©n√©rales utilis√©es pour concevoir des algorithmes efficaces. Ils permettent de r√©soudre une large gamme de probl√®mes en adoptant des approches sp√©cifiques adapt√©es √† la structure du probl√®me. Parmi les paradigmes les plus connus, on retrouve :

- **Diviser pour r√©gner**  
- **Programmation dynamique**  
- **Algorithmes gloutons**  
- **Retour sur trace (Backtracking)**  
- **Recherche locale et heuristiques**  

Nous allons explorer en d√©tail le premier paradigme : **Diviser pour r√©gner**.

---

## 4.1. Diviser pour r√©gner

Le paradigme **Diviser pour r√©gner** (Divide and Conquer) consiste √† r√©soudre un probl√®me en le d√©composant en sous-probl√®mes plus petits, en r√©solvant ces sous-probl√®mes de mani√®re r√©cursive, puis en combinant leurs solutions pour obtenir la solution globale.

### Principe g√©n√©ral

Un algorithme bas√© sur **Diviser pour r√©gner** suit g√©n√©ralement trois √©tapes :

1. **Diviser** : Le probl√®me est divis√© en plusieurs sous-probl√®mes plus petits (g√©n√©ralement de taille √©quivalente).  
2. **R√©gner** : Les sous-probl√®mes sont r√©solus de mani√®re r√©cursive.  
3. **Combiner** : Les solutions des sous-probl√®mes sont fusionn√©es pour obtenir la solution finale.

### Complexit√© et analyse

Si nous notons :
- **T(n)** : le temps d'ex√©cution de l'algorithme pour un probl√®me de taille **n**,
- **a** : le nombre de sous-probl√®mes cr√©√©s √† chaque √©tape,
- **b** : le facteur par lequel la taille des sous-probl√®mes est r√©duite,

alors la complexit√© suit g√©n√©ralement la **r√©currence de Master** :

\[
T(n) = aT(n/b) + f(n)
\]

o√π **f(n)** repr√©sente le co√ªt du travail effectu√© en dehors des appels r√©cursifs (par exemple, la fusion des solutions).

### Exemples d'algorithmes utilisant ce paradigme

- **Tri fusion (Merge Sort)**  
  - Divise le tableau en deux sous-tableaux de tailles √©gales.
  - Trie r√©cursivement chaque sous-tableau.
  - Fusionne les sous-tableaux tri√©s.
  - Complexit√© : **O(n log n)**.

- **Tri rapide (Quick Sort)**  
  - Choisit un pivot et partitionne le tableau en deux sous-tableaux.
  - Trie r√©cursivement les sous-tableaux.
  - Complexit√© moyenne : **O(n log n)** (pire cas **O(n¬≤)**).

- **Recherche dichotomique (Binary Search)**  
  - Divise le tableau en deux parties √©gales.
  - V√©rifie si l'√©l√©ment recherch√© est dans la premi√®re ou la seconde moiti√©.
  - R√©duit la taille du probl√®me par un facteur **2** √† chaque √©tape.
  - Complexit√© : **O(log n)**.

- **Algorithme de Strassen (multiplication de matrices)**  
  - D√©compose la multiplication de matrices en 7 multiplications de sous-matrices au lieu de 8.
  - Complexit√© : **O(n^{2.81})**, am√©liorant l'algorithme na√Øf **O(n¬≥)**.

---

### Avantages et inconv√©nients

‚úÖ **Avantages :**  
- Efficace pour les probl√®mes r√©cursifs.  
- Permet des solutions optimis√©es avec une complexit√© logarithmique ou quasi-lin√©aire.  
- Exploite bien la parall√©lisation.

‚ùå **Inconv√©nients :**  
- Peut entra√Æner un **co√ªt en m√©moire √©lev√©** d√ª aux appels r√©cursifs et au stockage temporaire des sous-probl√®mes.  
- Le choix d'une bonne strat√©gie de division est essentiel pour l'efficacit√©.  

---

Le paradigme **Diviser pour r√©gner** est une technique puissante utilis√©e dans de nombreux algorithmes de tri, de recherche et d'optimisation. D'autres paradigmes comme la **programmation dynamique** ou les **algorithmes gloutons** offrent des alternatives selon la nature du probl√®me.

## 4.2. Programmation Dynamique

### 1. Introduction √† la Programmation Dynamique

La **programmation dynamique** est une technique d'optimisation qui permet de r√©soudre des probl√®mes en les d√©composant en sous-probl√®mes plus petits et en stockant les r√©sultats interm√©diaires pour √©viter des calculs redondants. Elle est particuli√®rement utile lorsque les sous-probl√®mes se r√©p√®tent dans le processus de r√©solution.

### 2. Principe Fondamental

Un probl√®me peut √™tre r√©solu par programmation dynamique si :
- Il poss√®de une **structure optimale** : une solution optimale du probl√®me global est compos√©e de solutions optimales des sous-probl√®mes.
- Il pr√©sente une **redondance des sous-probl√®mes** : les m√™mes sous-probl√®mes apparaissent plusieurs fois.

### 3. Approches de la Programmation Dynamique

Il existe deux approches principales :

#### a) **Approche Top-Down (M√©mo√Øsation)**
Cette approche repose sur la r√©cursivit√© avec stockage des r√©sultats des sous-probl√®mes d√©j√† calcul√©s. Cela √©vite de recalculer les m√™mes sous-probl√®mes plusieurs fois.

**Exemple : Fibonacci avec m√©mo√Øsation**
```python
# Impl√©mentation en Python avec m√©mo√Øsation
def fibonacci(n, memo={}):
    if n in memo:
        return memo[n]
    if n <= 1:
        return n
    memo[n] = fibonacci(n-1, memo) + fibonacci(n-2, memo)
    return memo[n]

print(fibonacci(10))  # R√©sultat : 55
```

#### b) **Approche Bottom-Up (Tabulation)**
L'approche bottom-up consiste √† r√©soudre les sous-probl√®mes en premier, puis √† construire progressivement la solution finale.

**Exemple : Fibonacci avec tabulation**
```python
# Impl√©mentation en Python avec tabulation
def fibonacci(n):
    if n <= 1:
        return n
    fib = [0, 1]
    for i in range(2, n + 1):
        fib.append(fib[i-1] + fib[i-2])
    return fib[n]

print(fibonacci(10))  # R√©sultat : 55
```

### 4. Exemples Classiques de Programmation Dynamique

#### a) **Probl√®me du Sac √† Dos (0/1 Knapsack)**
Un voleur a un sac d'une certaine capacit√© et doit choisir des objets avec des valeurs et des poids diff√©rents afin d'obtenir la valeur maximale sans d√©passer la capacit√© du sac.

#### b) **Plus Longue Sous-S√©quence Commune (LCS - Longest Common Subsequence)**
Ce probl√®me consiste √† trouver la plus longue sous-s√©quence commune entre deux cha√Ænes de caract√®res.

#### c) **Probl√®me du Rendu de Monnaie**
Trouver le nombre minimal de pi√®ces pour donner une somme sp√©cifique en utilisant des pi√®ces de valeurs donn√©es.

### 5. Complexit√© et Optimisation

L'utilisation de la programmation dynamique permet souvent de r√©duire la complexit√© exponentielle √† une complexit√© polynomiale. Cependant, il faut faire attention √† la consommation m√©moire. Des techniques comme la **programmation dynamique en espace optimis√©** permettent de r√©duire l'utilisation de m√©moire en stockant uniquement les r√©sultats n√©cessaires.

### 6. Conclusion

La programmation dynamique est un outil puissant pour r√©soudre une large gamme de probl√®mes en informatique. Bien que son impl√©mentation n√©cessite une analyse approfondie des sous-probl√®mes et de la structure optimale, elle permet d'obtenir des solutions efficaces en optimisant les performances computationnelles.

## 4.3. Algorithmes Gloutons

### D√©finition
Un **algorithme glouton** (ou **greedy algorithm**) est une strat√©gie algorithmique qui fait des choix successifs en s√©lectionnant, √† chaque √©tape, l'option qui semble √™tre la meilleure √† court terme, sans tenir compte des cons√©quences futures.

### Principe de fonctionnement
L'approche gloutonne suit g√©n√©ralement ces √©tapes :
1. **S√©lection d‚Äôun choix optimal localement** : choisir la meilleure option imm√©diate disponible.
2. **V√©rification de la faisabilit√©** : s'assurer que le choix respecte les contraintes du probl√®me.
3. **Construction progressive d‚Äôune solution** : r√©p√©ter le processus jusqu'√† obtenir une solution compl√®te.

### Conditions d‚Äôapplicabilit√©
Un algorithme glouton donne une solution optimale uniquement si le probl√®me satisfait **l‚Äôune des deux propri√©t√©s suivantes** :
- **Propri√©t√© de sous-structure optimale** : une solution optimale du probl√®me global peut √™tre obtenue √† partir de solutions optimales de sous-probl√®mes.
- **Propri√©t√© du choix glouton** : un choix optimal local conduit toujours √† une solution optimale globale.

Si ces conditions ne sont pas remplies, l‚Äôalgorithme peut produire une solution sous-optimale.

### Exemples d‚Äôalgorithmes gloutons

#### 1. **Probl√®me du rendu de monnaie**
L‚Äôobjectif est de rendre une somme donn√©e avec le moins de pi√®ces possible. Un algorithme glouton choisit √† chaque √©tape la pi√®ce de plus grande valeur disponible.

**Exemple en euros :**
- Somme √† rendre : 63 centimes
- Pi√®ces disponibles : {50, 20, 10, 5, 2, 1} centimes
- Solution gloutonne : 50 + 10 + 2 + 1 (4 pi√®ces)

‚ö†Ô∏è **Attention** : Cet algorithme ne fonctionne pas toujours pour des syst√®mes mon√©taires o√π des combinaisons non triviales donnent un meilleur r√©sultat.

#### 2. **Probl√®me du sac √† dos fractionnaire (Knapsack Fractionnaire)**
- Un voleur doit choisir des objets √† mettre dans un sac √† dos de capacit√© limit√©e.
- Chaque objet a un poids et une valeur.
- L‚Äôobjectif est de maximiser la valeur totale des objets dans le sac.
- Un algorithme glouton prend toujours l‚Äôobjet ayant **le meilleur rapport valeur/poids** en premier.
- Pour le cas fractionnaire (o√π l‚Äôon peut prendre une partie d‚Äôun objet), la solution gloutonne est **optimale**.

#### 3. **Algorithme de Dijkstra**
L‚Äôalgorithme de Dijkstra, utilis√© pour trouver le plus court chemin dans un graphe pond√©r√©, suit une approche gloutonne en choisissant **le sommet ayant la plus petite distance courante**.

#### 4. **Algorithme de Prim**
L‚Äôalgorithme de Prim construit un **arbre couvrant minimal** en ajoutant √† chaque √©tape l‚Äôar√™te de plus faible poids connectant un nouveau sommet √† l‚Äôarbre en construction.

### Avantages et inconv√©nients
‚úÖ **Avantages**
- Facile √† comprendre et impl√©menter.
- Rapide et efficace pour certains probl√®mes.
- Fonctionne bien lorsque les propri√©t√©s optimales sont v√©rifi√©es.

‚ùå **Inconv√©nients**
- Ne garantit pas toujours une solution optimale.
- Peut n√©cessiter une preuve formelle pour v√©rifier sa validit√©.
- Certains probl√®mes n√©cessitent une approche plus avanc√©e comme la programmation dynamique.

### Conclusion
Les algorithmes gloutons sont puissants pour certains types de probl√®mes mais ne sont pas universellement applicables. Ils sont souvent utilis√©s lorsque les d√©cisions locales garantissent une solution optimale globale. Lorsqu‚Äôils √©chouent, il est souvent n√©cessaire d‚Äôavoir recours √† des approches comme la **programmation dynamique** ou la **recherche exhaustive**.

## 4.4. Retour sur trace (Backtracking)

### 1. Introduction au Backtracking
Le **retour sur trace** (ou *backtracking*) est une technique algorithmique qui explore toutes les solutions possibles √† un probl√®me en construisant une solution incr√©mentalement. Lorsqu'une branche explor√©e m√®ne √† une impasse, l'algorithme revient en arri√®re ("backtrack") pour explorer une autre possibilit√©.

### 2. Principe du Backtracking
Le backtracking suit un sch√©ma r√©cursif o√π l'on :
1. Construit une solution partielle.
2. V√©rifie si elle satisfait les contraintes du probl√®me.
3. Si oui, on poursuit avec l'√©tape suivante.
4. Si non, on revient en arri√®re pour essayer une autre possibilit√©.

Le **backtracking** est particuli√®rement efficace pour les probl√®mes combinatoires, o√π l'on cherche √† g√©n√©rer toutes les solutions possibles et √† en valider certaines.

### 3. Exemples d'Applications
Le **backtracking** est utilis√© dans plusieurs domaines, notamment :
- **Le probl√®me des huit reines** (placer 8 reines sur un √©chiquier sans qu'elles ne s'attaquent)
- **Le Sudoku** (remplir une grille en respectant les contraintes)
- **Le probl√®me du sac √† dos** (optimisation combinatoire)
- **Les labyrinthes** (trouver un chemin dans un graphe)

### 4. Impl√©mentation en Python
Voici un exemple simple du backtracking appliqu√© au **probl√®me des N reines** :

```python
def est_safe(echiquier, ligne, col, n):
    for i in range(col):
        if echiquier[ligne][i] == 1:
            return False
    for i, j in zip(range(ligne, -1, -1), range(col, -1, -1)):
        if echiquier[i][j] == 1:
            return False
    for i, j in zip(range(ligne, n, 1), range(col, -1, -1)):
        if echiquier[i][j] == 1:
            return False
    return True

def resoudre_n_reines(echiquier, col, n):
    if col >= n:
        return True
    for i in range(n):
        if est_safe(echiquier, i, col, n):
            echiquier[i][col] = 1
            if resoudre_n_reines(echiquier, col + 1, n):
                return True
            echiquier[i][col] = 0  # Backtrack
    return False

def n_reines(n):
    echiquier = [[0] * n for _ in range(n)]
    if not resoudre_n_reines(echiquier, 0, n):
        print("Solution inexistante")
    else:
        for ligne in echiquier:
            print(" ".join(str(x) for x in ligne))

n_reines(8)
```

### 5. Complexit√© du Backtracking
La complexit√© du backtracking d√©pend du probl√®me trait√©. Dans le pire des cas, il peut explorer toutes les solutions possibles, ce qui donne une complexit√© exponentielle **O(k^n)** pour un probl√®me combinatoire √† **n** √©tapes et **k** choix possibles √† chaque √©tape.

Cependant, des optimisations comme **la branche et l‚Äô√©lagage (branch and bound)** ou l‚Äô**ordre de recherche heuristique** peuvent am√©liorer l'efficacit√©.

### 6. Conclusion
Le **backtracking** est une m√©thode puissante pour r√©soudre des probl√®mes combinatoires et de recherche. Bien que souvent co√ªteux en termes de temps d'ex√©cution, il reste une approche essentielle lorsqu'une solution exacte est requise et que l'exploration exhaustive est envisageable.

## 4.5. Recherche locale et heuristiques

### 4.5.1. Introduction
La **recherche locale** et les **heuristiques** sont des approches permettant de trouver des solutions approximatives √† des probl√®mes d‚Äôoptimisation difficiles, souvent NP-durs. Contrairement aux m√©thodes exactes, ces approches ne garantissent pas n√©cessairement une solution optimale mais fournissent une solution satisfaisante en un temps raisonnable.

---

### 4.5.2. Recherche locale
La recherche locale explore l‚Äôespace des solutions en passant d‚Äôune solution √† une autre par de petites modifications appel√©es **mouvements**. Elle est efficace pour les probl√®mes combinatoires comme le **voyageur de commerce** (TSP) ou le **probl√®me de satisfaction de contraintes**.

#### Principes :
- D√©finition d‚Äôun **espace de solutions**.
- Utilisation d‚Äôune **fonction de co√ªt** pour √©valuer la qualit√© des solutions.
- Application d‚Äôun **mouvement local** pour passer d‚Äôune solution √† une autre.

#### Exemples d‚Äôalgorithmes de recherche locale :
1. **Descente de gradient (Hill Climbing)**  
   - √Ä chaque √©tape, on passe √† la meilleure solution voisine.
   - Risque de rester bloqu√© dans un **optimum local**.

2. **Recuit simul√© (Simulated Annealing)**  
   - Inspir√© du processus physique de refroidissement des m√©taux.
   - Introduit une probabilit√© d‚Äôaccepter des solutions moins bonnes pour √©viter les optima locaux.

3. **Recherche tabou (Tabu Search)**  
   - Maintient une liste des solutions r√©centes pour √©viter de revenir en arri√®re.
   - Permet d‚Äôexplorer plus efficacement l‚Äôespace des solutions.

---

### 4.5.3. Heuristiques
Les **heuristiques** sont des strat√©gies permettant de produire rapidement une solution satisfaisante en sacrifiant parfois l‚Äôoptimalit√©. Elles sont souvent sp√©cifiques √† un probl√®me donn√©.

#### Types d‚Äôheuristiques :
1. **Algorithmes gloutons (Greedy Algorithms)**  
   - S√©lectionnent localement la meilleure option sans consid√©ration du futur.
   - Exemples : l‚Äôalgorithme de Prim pour les arbres couvrants, Dijkstra pour les plus courts chemins.

2. **M√©thodes bas√©es sur des r√®gles (Rule-based Methods)**  
   - Utilisation de r√®gles heuristiques sp√©cifiques au probl√®me.
   - Exemple : heuristique du plus proche voisin pour le probl√®me du voyageur de commerce.

3. **Algorithmes √©volutionnaires et m√©taheuristiques**  
   - Inspir√©s des processus biologiques ou physiques.
   - Exemples : algorithmes g√©n√©tiques, colonies de fourmis, optimisation par essaim de particules.

---

### 4.5.4. Comparaison et Applications
| M√©thode                | Avantages                            | Inconv√©nients                      | Exemples d‚Äôapplication |
|------------------------|------------------------------------|------------------------------------|------------------------|
| Descente de gradient   | Simple, efficace pour certains probl√®mes | Bloqu√© dans les optima locaux | Probl√®mes de clustering, optimisation continue |
| Recuit simul√©         | √âvite les optima locaux, adaptable | Param√©trage d√©licat | Probl√®mes combinatoires |
| Recherche tabou       | Exploration plus large de l‚Äôespace | Co√ªt m√©moire plus √©lev√© | Planification, ordonnancement |
| Algorithmes gloutons  | Rapide, impl√©mentation simple | Ne donne pas toujours la solution optimale | Graphes, optimisation de r√©seau |
| Algorithmes √©volutionnaires | Bonne exploration globale | Temps de calcul √©lev√© | Machine learning, optimisation multi-objectifs |

---

### 4.5.5. Conclusion
Les algorithmes de **recherche locale** et les **heuristiques** sont essentiels pour r√©soudre efficacement des probl√®mes complexes. Bien que ces approches ne garantissent pas toujours l‚Äôoptimalit√©, elles sont largement utilis√©es en intelligence artificielle, en recherche op√©rationnelle et en optimisation.

---
# 5. Structures de Donn√©es et Algorithmes Fondamentaux

## 5.1. Listes, Piles et Files

Les structures de donn√©es jouent un r√¥le crucial dans la conception des algorithmes. Parmi les plus fondamentales, on retrouve les **listes**, **piles** et **files**.

### 5.1.1. Listes
Une **liste** est une structure de donn√©es lin√©aire permettant de stocker une collection d‚Äô√©l√©ments. On distingue principalement :
- **Liste cha√Æn√©e** : chaque √©l√©ment (n≈ìud) contient une valeur et un pointeur vers l‚Äô√©l√©ment suivant.
- **Liste doublement cha√Æn√©e** : chaque √©l√©ment contient un pointeur vers l‚Äô√©l√©ment pr√©c√©dent et suivant.
- **Liste circulaire** : le dernier √©l√©ment pointe vers le premier.

üìå *Avantages* :
- Insertion et suppression rapides (O(1) pour une liste cha√Æn√©e).
- Pas de taille fixe, contrairement aux tableaux.

üìå *Inconv√©nients* :
- Acc√®s plus lent aux √©l√©ments (O(n) en moyenne).
- Surcharge m√©moire due aux pointeurs.

### 5.1.2. Piles (*Stack*)
Une **pile** est une structure de donn√©es respectant le principe **LIFO** (*Last In, First Out*), o√π le dernier √©l√©ment ajout√© est le premier retir√©.

**Op√©rations principales** :
- `push(x)`: ajoute un √©l√©ment `x` au sommet.
- `pop()`: retire l‚Äô√©l√©ment au sommet.
- `peek()`: consulte l‚Äô√©l√©ment au sommet sans le retirer.

üìå *Applications* :
- Gestion des appels de fonctions (pile d‚Äôex√©cution).
- Annulation d‚Äôactions (Ctrl + Z).
- √âvaluation d‚Äôexpressions (notation postfixe).

### 5.1.3. Files (*Queue*)
Une **file** suit le principe **FIFO** (*First In, First Out*), o√π le premier √©l√©ment ajout√© est le premier retir√©.

**Types de files** :
- **File simple** : ajout √† l‚Äôarri√®re (*enqueue*), retrait √† l‚Äôavant (*dequeue*).
- **File double (*Deque*)** : insertion et suppression possibles aux deux extr√©mit√©s.
- **File de priorit√©** : les √©l√©ments sont extraits selon une priorit√© et non leur ordre d‚Äôarriv√©e.

üìå *Applications* :
- Gestion des t√¢ches dans les syst√®mes d‚Äôexploitation.
- Algorithmes de parcours en largeur (*BFS*).
- Impression de documents en file d‚Äôattente.

---

Dans les sections suivantes, nous explorerons d‚Äôautres structures de donn√©es avanc√©es comme les **arbres** et les **graphes**, ainsi que leurs algorithmes associ√©s.
## 5.2. Arbres et Graphes

### 5.2.1. D√©finition et Terminologie

Un **graphe** est une structure compos√©e de **sommets** (ou n≈ìuds) et d'**ar√™tes** (ou arcs) reliant ces sommets. On distingue plusieurs types de graphes :

- **Graphe orient√©** : les ar√™tes ont une direction.
- **Graphe non orient√©** : les ar√™tes n'ont pas de direction.
- **Graphe pond√©r√©** : chaque ar√™te est associ√©e √† un poids.
- **Graphe connexe** : il existe un chemin entre chaque paire de sommets.

Un **arbre** est un cas particulier de graphe :

- C'est un graphe acyclique connexe.
- Un arbre avec \( n \) sommets poss√®de exactement \( n-1 \) ar√™tes.
- Il poss√®de une hi√©rarchie avec un sommet racine (dans le cas d'un arbre enracin√©).

### 5.2.2. Repr√©sentation des Graphes

Il existe plusieurs mani√®res de repr√©senter un graphe en informatique :

1. **Liste d‚Äôadjacence** : chaque sommet est associ√© √† une liste de ses voisins.
2. **Matrice d‚Äôadjacence** : une matrice \( n \times n \) o√π la case \( (i, j) \) indique la pr√©sence ou l'absence d'une ar√™te entre les sommets \( i \) et \( j \).

Exemple d‚Äôune liste d‚Äôadjacence pour un graphe non orient√© :

```
1 ‚Üí [2, 3]
2 ‚Üí [1, 4]
3 ‚Üí [1, 4]
4 ‚Üí [2, 3]
```

### 5.2.3. Parcours des Graphes

Les algorithmes de parcours permettent d‚Äôexplorer un graphe :

- **Parcours en largeur (BFS - Breadth-First Search)** : explore les sommets niveau par niveau.
- **Parcours en profondeur (DFS - Depth-First Search)** : explore les sommets en profondeur avant de revenir en arri√®re.

#### Impl√©mentation du BFS en Python

```python
from collections import deque

def bfs(graph, start):
    visited = set()
    queue = deque([start])
    
    while queue:
        node = queue.popleft()
        if node not in visited:
            print(node, end=" ")
            visited.add(node)
            queue.extend(graph[node])
```

### 5.2.4. Applications des Graphes et Arbres

Les arbres et graphes sont utilis√©s dans de nombreux domaines :

- **Informatique** : mod√©lisation des r√©seaux, intelligence artificielle.
- **Bioinformatique** : classification phylog√©n√©tique.
- **Transport** : planification des itin√©raires.
- **Web** : structure des liens entre les pages (PageRank de Google).

## 5.3. Algorithmes de tri (tri fusion, rapide, tas, etc.)

Les algorithmes de tri sont essentiels en informatique, car ils permettent de r√©organiser les √©l√©ments d'un tableau ou d'une liste selon un ordre donn√© (croissant ou d√©croissant). Plusieurs algorithmes existent, chacun ayant ses propres caract√©ristiques en termes de complexit√© temporelle et d'usage. Voici les algorithmes les plus utilis√©s :

### 1. Tri par fusion (Merge Sort)
Le tri par fusion est un algorithme de tri bas√© sur le paradigme "Diviser pour r√©gner". Il divise r√©cursivement le tableau en sous-tableaux de plus en plus petits jusqu'√† obtenir des sous-tableaux de taille 1 (qui sont d√©j√† tri√©s). Ensuite, il fusionne les sous-tableaux tri√©s pour obtenir le tableau final tri√©.

#### Complexit√© :
- **Complexit√© temporelle** : O(n log n)
- **Complexit√© spatiale** : O(n) (en raison de la m√©moire suppl√©mentaire utilis√©e pour stocker les sous-tableaux)

#### Fonctionnement :
1. Diviser le tableau en deux moiti√©s.
2. Trier chaque moiti√© r√©cursivement.
3. Fusionner les deux moiti√©s tri√©es pour obtenir un tableau tri√©.

### 2. Tri rapide (Quick Sort)
Le tri rapide est √©galement un algorithme de tri "Diviser pour r√©gner". Il s√©lectionne un "pivot" dans le tableau, partitionne le tableau en deux sous-tableaux (un avec des √©l√©ments plus petits que le pivot et l'autre avec des √©l√©ments plus grands) et trie ces sous-tableaux de mani√®re r√©cursive.

#### Complexit√© :
- **Complexit√© temporelle moyenne** : O(n log n)
- **Complexit√© temporelle dans le pire cas** : O(n¬≤) (cela peut √™tre √©vit√© en choisissant un bon pivot)
- **Complexit√© spatiale** : O(log n) en moyenne

#### Fonctionnement :
1. Choisir un pivot dans le tableau.
2. Partitionner le tableau en fonction du pivot (les √©l√©ments plus petits que le pivot √† gauche et les plus grands √† droite).
3. Trier r√©cursivement les sous-tableaux √† gauche et √† droite du pivot.

### 3. Tri par tas (Heap Sort)
Le tri par tas utilise une structure de donn√©es appel√©e "tas" (ou "heap"). Un tas est un arbre binaire complet qui satisfait √† la propri√©t√© de tas (le parent est plus grand ou plus petit que ses enfants, selon que l'on utilise un tas maximum ou minimum). Le tri par tas transforme d'abord le tableau en un tas, puis extrait les √©l√©ments un √† un en ajustant le tas √† chaque extraction.

#### Complexit√© :
- **Complexit√© temporelle** : O(n log n)
- **Complexit√© spatiale** : O(1)

#### Fonctionnement :
1. Convertir le tableau en un tas.
2. Extraire l'√©l√©ment racine du tas (le plus grand ou le plus petit) et le placer √† la fin du tableau.
3. R√©ajuster le tas et r√©p√©ter jusqu'√† ce que tous les √©l√©ments soient tri√©s.

### 4. Tri par insertion (Insertion Sort)
Le tri par insertion est un algorithme simple o√π les √©l√©ments sont ins√©r√©s un √† un dans une portion tri√©e du tableau. L'√©l√©ment actuel est compar√© avec les √©l√©ments d√©j√† tri√©s et est ins√©r√© √† la bonne position.

#### Complexit√© :
- **Complexit√© temporelle dans le pire cas** : O(n¬≤)
- **Complexit√© temporelle dans le meilleur cas** : O(n) (lorsque le tableau est d√©j√† tri√©)
- **Complexit√© spatiale** : O(1)

#### Fonctionnement :
1. Commencer avec un tableau vide.
2. Parcourir le tableau et ins√©rer chaque √©l√©ment dans la partie tri√©e du tableau en d√©calant les √©l√©ments n√©cessaires.

### 5. Tri par s√©lection (Selection Sort)
Le tri par s√©lection fonctionne en s√©lectionnant successivement l'√©l√©ment le plus petit (ou le plus grand) dans la portion non tri√©e du tableau et en l'√©changeant avec l'√©l√©ment en d√©but de la portion non tri√©e.

#### Complexit√© :
- **Complexit√© temporelle** : O(n¬≤)
- **Complexit√© spatiale** : O(1)

#### Fonctionnement :
1. Trouver l'√©l√©ment le plus petit dans le tableau non tri√©.
2. L'√©changer avec le premier √©l√©ment du tableau non tri√©.
3. R√©p√©ter ce processus pour chaque position du tableau.

### 6. Tri Shell (Shell Sort)
Le tri Shell est une am√©lioration du tri par insertion. Il permet de trier les √©l√©ments en utilisant une s√©quence de pas (gap) et en ins√©rant les √©l√©ments dans des sous-listes d√©finies par ces pas. Cela permet d'am√©liorer les performances par rapport au tri par insertion classique.

#### Complexit√© :
- **Complexit√© temporelle** : Varie en fonction de la s√©quence des pas, mais peut √™tre O(n^(3/2)) ou O(n log¬≤ n) dans le meilleur des cas.
- **Complexit√© spatiale** : O(1)

#### Fonctionnement :
1. S√©lectionner une s√©quence de pas.
2. Trier les √©l√©ments en utilisant ces pas, en ins√©rant dans des sous-listes d√©finies par les pas.
3. R√©duire progressivement les pas jusqu'√† atteindre 1, ce qui revient au tri par insertion classique.

### Conclusion
Les algorithmes de tri sont des outils fondamentaux en informatique, et le choix de l'algorithme d√©pend du contexte (taille du tableau, structure des donn√©es, etc.). Les algorithmes comme le tri fusion et le tri rapide sont largement utilis√©s en raison de leur efficacit√©, tandis que des algorithmes comme le tri par insertion et le tri par s√©lection peuvent √™tre utiles pour de petites tailles de donn√©es ou des cas particuliers.

